{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwK3wRUjgTkE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Neural Architecture Search (Dummy Data)"
      ],
      "metadata": {
        "id": "qJtRGLCHH6oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchableAttention(nn.Module):\n",
        "  #Searcheable multi-head attention with architecture choices\n",
        "  def __init__(self, d_model: int, max_heads: int = 8):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_heads = max_heads\n",
        "\n",
        "    #Architecture parameters (Learnable)\n",
        "    self.alpha_heads = nn.Parameter(torch.randn(max_heads))\n",
        "    self.alpha_patterns = nn.Parameter(torch.randn(3)) # full, local, sparse\n",
        "\n",
        "    #Attention Components for Diff head counts\n",
        "    self.attentions = nn.ModuleList([nn.MultiheadAttention(d_model, num_heads=i+1, batch_first=True) for i in range(max_heads)])\n",
        "\n",
        "  def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "    #Softmax over architecture choices\n",
        "    head_weights = F.softmax(self.alpha_heads, dim=0)\n",
        "    pattern_weights = F.softmax(self.alpha_patterns, dim=0)\n",
        "\n",
        "    #weighted combination of diff attention heads\n",
        "    outputs = []\n",
        "    for i, attn in enumerate(self.attentions):\n",
        "      out, _ = attn(x, x, x, attn_mask=mask)\n",
        "      outputs.append(head_weights[i] * out)\n",
        "\n",
        "    combined = sum(outputs)\n",
        "\n",
        "    #Apply diff attention patterns (simplified)\n",
        "    full_attn = combined\n",
        "    local_attn = combined * 0.8 # Simulate local attention\n",
        "    sparse_attn = combined * 0.6 # Simulate Sparse attention\n",
        "\n",
        "    final_output = (pattern_weights[0] * full_attn + pattern_weights[1] * local_attn + pattern_weights[2] * sparse_attn)\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "lFT4q0E2g7r3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "  #Softmax over architecture choices\n",
        "  head_weights = F.softmax(self.alpha_heads, dim=0)\n",
        "  pattern_weights = F.softmax(self.alpha_patterns, dim=0)\n",
        "\n",
        "  #weighted combination of diff attention heads\n",
        "  outputs = []\n",
        "  for i, attn in enumerate(self.attentions):\n",
        "    out, _ = attn(x, x, x, attn_mask=mask)\n",
        "    outputs.append(head_weights[i] * out)\n",
        "\n",
        "  combined = sum(outputs)\n",
        "\n",
        "  #Apply diff attention patterns (simplified)\n",
        "  full_attn = combined\n",
        "  local_attn = combined * 0.8 # Simulate local attention\n",
        "  sparse_attn = combined * 0.6 # Simulate Sparse attention\n",
        "\n",
        "  final_output = (pattern_weights[0] * full_attn + pattern_weights[1] * local_attn + pattern_weights[2] * sparse_attn)\n",
        "  return final_output"
      ],
      "metadata": {
        "id": "E8kf4tyClR0B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchableFFN(nn.Module):\n",
        "  #Searchable feed-forward network with diff expansion ratios\n",
        "  def __init__(self, d_model: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "\n",
        "    #Architecture Parameters for diff expansion ratios\n",
        "    self.alpha_expansion = nn.Parameter(torch.randn(4))\n",
        "\n",
        "    #Diff FFN configurations\n",
        "    self.ffns = nn.ModuleList([\n",
        "    self._make_ffn(d_model, int(d_model * ratio))\n",
        "    for ratio in [2,4,6,8]])\n",
        "\n",
        "  def _make_ffn(self, d_in: int, d_hidden: int):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(d_in, d_hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_hidden, d_in)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    weights = F.softmax(self.alpha_expansion, dim=0)\n",
        "    outputs = []\n",
        "    for i, ffn in enumerate(self.ffns):\n",
        "        outputs.append(weights[i] * ffn(x))\n",
        "    return sum(outputs)"
      ],
      "metadata": {
        "id": "v1cv5xBRnVAL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchableTransformerBlock(nn.Module):\n",
        "  #Searchable transformer block with multiple architectural choices\n",
        "\n",
        "  def __init__(self, d_model: int, max_heads: int = 8, dropout: float =0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "\n",
        "    #Architecture choice: skip connection patterns\n",
        "    self.alpha_skip = nn.Parameter(torch.rand(3))\n",
        "\n",
        "    self.attention = SearchableAttention(d_model, max_heads)\n",
        "    self.ffn = SearchableFFN(d_model)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "    #Softmax over skip connection patterns\n",
        "    skip_weights = F.softmax(self.alpha_skip, dim=0)\n",
        "\n",
        "    #Diff normalization Patterns\n",
        "    #Pre Norm\n",
        "\n",
        "    prenorm_x = self.norm1(x)\n",
        "    prenorm_attn = self.attention(prenorm_x, mask)\n",
        "    prenorm_out = self.dropout(prenorm_attn)\n",
        "    prenorm_ffn = self.ffn(self.norm2(prenorm_out))\n",
        "    prenorm_final = prenorm_out + self.dropout(prenorm_ffn)\n",
        "\n",
        "    #Post Norm\n",
        "    postnorm_attn= self.attention(x, mask)\n",
        "    postnorm_out = self.norm1(x + self.dropout(postnorm_attn)) # Corrected typo\n",
        "    postnorm_ffn = self.ffn(postnorm_out)\n",
        "    postnorm_final = self.norm2(postnorm_out + self.dropout(postnorm_ffn))\n",
        "\n",
        "    #No norm (residual only)\n",
        "    nonorm_attn = self.attention(x, mask)\n",
        "    nonorm_out = x + self.dropout(nonorm_attn)\n",
        "    nonorm_ffn = self.ffn(nonorm_out)\n",
        "    nonorm_final = nonorm_out + self.dropout(nonorm_ffn)\n",
        "\n",
        "    # Weighted combination\n",
        "    return (skip_weights[0] * prenorm_final +\n",
        "            skip_weights[1] * postnorm_final +\n",
        "            skip_weights[2] * nonorm_final)"
      ],
      "metadata": {
        "id": "ag90dqPXI3Lm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchableLLM(nn.Module):\n",
        "\n",
        "  # Neural Architecture Search enabled Language Model\n",
        "\n",
        "  def __init__(self, vocab_size: int,\n",
        "               d_model: int = 512,\n",
        "               max_layers: int = 12,\n",
        "               max_heads: int = 8,\n",
        "               max_seq_len: int = 1024,\n",
        "               dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.max_layers = max_layers\n",
        "\n",
        "    #Architecture Parameters for Layer Count\n",
        "    self.alpha_layers = nn.Parameter(torch.randn(max_layers)) # Corrected typo\n",
        "\n",
        "    #Embeddings\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "    #Searchable transformer blocks\n",
        "    self.blocks = nn.ModuleList([\n",
        "        SearchableTransformerBlock(d_model, max_heads, dropout)\n",
        "        for _ in range(max_layers)\n",
        "    ])\n",
        "\n",
        "    #Output Projection\n",
        "    self.norm_f = nn.LayerNorm(d_model) # Corrected typo\n",
        "    self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
        "    batch_size, seq_length = input_ids.shape\n",
        "    device = input_ids.device\n",
        "\n",
        "    #Positional Encoding (EMBEDDINGS)\n",
        "    positions = torch.arange(seq_length, device=device).unsqueeze(0)\n",
        "    x = self.token_embedding(input_ids) + self.pos_embedding(positions)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    #Weighted combination of diff layer depths\n",
        "    layer_weights = F.softmax(self.alpha_layers, dim=0)\n",
        "\n",
        "    #Process through all blocks but weight their contributions\n",
        "    layer_outputs = [x]\n",
        "    current_x = x\n",
        "\n",
        "    for i, block in enumerate(self.blocks):\n",
        "      current_x = block(current_x, attention_mask)\n",
        "      layer_outputs.append(current_x)\n",
        "\n",
        "    #Weighted sum of outputs from diff depths\n",
        "    weighted_output = sum(layer_weights[i] * layer_outputs[i+1]\n",
        "                          for i in range(len(self.blocks)))\n",
        "\n",
        "    x = self.norm_f(weighted_output) # Corrected typo\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def get_architecture_params(self):\n",
        "    #Get all architecture parameters for optmization\n",
        "    arch_params = []\n",
        "    arch_params.append(self.alpha_layers)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      arch_params.append(block.alpha_skip)\n",
        "      arch_params.append(block.attention.alpha_heads)\n",
        "      arch_params.append(block.attention.alpha_patterns)\n",
        "      arch_params.append(block.ffn.alpha_expansion)\n",
        "\n",
        "    return arch_params\n",
        "\n",
        "  def get_model_params(self):\n",
        "    # Get all model parameters (excluding architecture params)\n",
        "    arch_params_ids = {id(p) for p in self.get_architecture_params()}\n",
        "    return [p for p in self.parameters() if id(p) not in arch_params_ids]"
      ],
      "metadata": {
        "id": "ibyZ66AFMsPo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NASTrainer:\n",
        "# Neural Architecture search trainer using DARTS methodology\n",
        "  def __init__(self,\n",
        "               model: SearchableLLM,\n",
        "               train_loader,\n",
        "               val_loader,\n",
        "               device: str = 'cuda'):\n",
        "    self.model = model.to(device)\n",
        "    self.train_loader = train_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "    # Seperate optimizers for model and architecture parameters\n",
        "    self.model_optimizer = optim.AdamW(model.get_model_params(),\n",
        "                                       lr=1e-4,\n",
        "                                       weight_decay=1e-4)\n",
        "    self.arch_optimizer = optim.Adam(model.get_architecture_params(), lr=3e-4, betas=(0.5, 0.999))\n",
        "\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def train_step(self, batch):\n",
        "    #Single training step\n",
        "    input_ids, labels = batch\n",
        "    input_ids, labels = input_ids.to(self.device), labels.to(self.device)\n",
        "\n",
        "    #shift labels for language modeling\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "    # Pass sliced input_ids to the model\n",
        "    shift_logits = self.model(input_ids[:, :-1])[..., :].contiguous()\n",
        "\n",
        "    loss = self.criterion(\n",
        "        shift_logits.view(-1, shift_logits.shape[-1]),\n",
        "        shift_labels.view(-1)\n",
        "    )\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def search_step(self):\n",
        "    # DARTS search step: optimize architecture parameters\n",
        "    self.model.train()\n",
        "\n",
        "    #Get validation batch for architecture optimization\n",
        "    try:\n",
        "      val_batch = next(iter(self.val_loader))\n",
        "    except StopIteration:\n",
        "      return 0.0\n",
        "\n",
        "    #Optimize architecture parameters on validation set\n",
        "    self.arch_optimizer.zero_grad()\n",
        "    val_loss = self.train_step(val_batch)\n",
        "    val_loss.backward()\n",
        "    self.arch_optimizer.step()\n",
        "\n",
        "    return val_loss.item()\n",
        "\n",
        "  def model_step(self):\n",
        "    #Optimize model parameters on training set\n",
        "    self.model.train()\n",
        "\n",
        "    try:\n",
        "      train_batch = next(iter(self.train_loader))\n",
        "    except StopIteration:\n",
        "      return 0.0\n",
        "\n",
        "    self.model_optimizer.zero_grad()\n",
        "    train_loss = self.train_step(train_batch)\n",
        "    train_loss.backward()\n",
        "    self.model_optimizer.step()\n",
        "\n",
        "    return train_loss.item()\n",
        "\n",
        "  def train_epoch(self):\n",
        "    #Train for one epoch alternating btw model and architecture optimization\n",
        "    total_model_loss = 0.0\n",
        "    total_arch_loss = 0.0\n",
        "    n_steps = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(self.train_loader): # Corrected unpacking\n",
        "      # Initialize loss variables for the current batch\n",
        "      model_loss = 0.0\n",
        "      arch_loss = 0.0\n",
        "\n",
        "      #Alternate btw model and architecture optimization\n",
        "      if batch_idx % 2 == 0:\n",
        "        model_loss = self.model_step()\n",
        "        total_model_loss += model_loss\n",
        "      else:\n",
        "        arch_loss = self.search_step()\n",
        "        total_arch_loss += arch_loss\n",
        "\n",
        "      n_steps += 1\n",
        "\n",
        "      if batch_idx % 100 == 0:\n",
        "        print(f\"Batch {batch_idx}, Model Loss: {model_loss:.4f}, Arch Loss: {arch_loss:.4f}\")\n",
        "\n",
        "    return total_model_loss / n_steps, total_arch_loss / n_steps\n",
        "\n",
        "  def get_final_architecture(self):\n",
        "    #extract the final discovered architecture\n",
        "    arch = {}\n",
        "\n",
        "    #layer depth\n",
        "    layer_probs = F.softmax(self.model.alpha_layers, dim=0)\n",
        "    arch['optimal_layers'] = torch.argmax(layer_probs).item() + 1\n",
        "    arch['layer_weights'] = layer_probs.detach().cpu().numpy()\n",
        "\n",
        "    #Block Level Choices\n",
        "    block_configs = []\n",
        "    for i, block in enumerate(self.model.blocks):\n",
        "      block_config = {}\n",
        "\n",
        "      #skip connection patterns\n",
        "      skip_probs = F.softmax(block.alpha_skip, dim=0)\n",
        "      skip_patterns = ['prenorm', 'postnorm', 'nonorm']\n",
        "      block_config['skip_pattern'] = skip_patterns[torch.argmax(skip_probs).item()]\n",
        "\n",
        "\n",
        "      #Attention heads\n",
        "      head_probs = F.softmax(block.attention.alpha_heads, dim=0)\n",
        "      block_config['num_heads'] = torch.argmax(head_probs).item() + 1\n",
        "\n",
        "      #Attention pattern\n",
        "      pattern_probs = F.softmax(block.attention.alpha_patterns, dim=0)\n",
        "      patterns = ['full', 'local', 'sparse']\n",
        "      block_config['attention_pattern'] = patterns[torch.argmax(pattern_probs).item()]\n",
        "\n",
        "      #FFN expansion\n",
        "      exp_probs = F.softmax(block.ffn.alpha_expansion, dim=0)\n",
        "      expansions = [2, 4, 6, 8]\n",
        "      block_config['ffn_expansion'] = expansions[torch.argmax(exp_probs).item()]\n",
        "\n",
        "      block_configs.append(block_config)\n",
        "\n",
        "    arch['block_configs'] = block_configs\n",
        "\n",
        "    return arch\n",
        "\n",
        "  #Example usage and utility functions\n",
        "  def create_dummy_data(vocab_size: int = 1000, seq_len: int = 128, batch_size: int = 8):\n",
        "    #Create Dummy data for testing\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "    labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "    return [(input_ids, labels)]"
      ],
      "metadata": {
        "id": "X3OXCh_Rr1DC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "984ed0f7",
        "outputId": "18d3ad8f-78f2-4b41-d38d-8906895f3cfe"
      },
      "source": [
        "def main():\n",
        "  #example usage of NAS for LLMs\n",
        "  #Configuration\n",
        "  vocab_size = 1000\n",
        "  d_model = 264 # Changed d_model to be divisible by 1, 2, 3, and 4\n",
        "  max_layers = 6\n",
        "  max_heads = 4\n",
        "  seq_len = 128\n",
        "\n",
        "  #create model\n",
        "  model = SearchableLLM(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    max_layers=max_layers,\n",
        "    max_heads=max_heads,\n",
        "    max_seq_len=seq_len,\n",
        "\n",
        "    )\n",
        "\n",
        "  #create dummy data\n",
        "  train_data = NASTrainer.create_dummy_data(vocab_size=vocab_size, seq_len=seq_len, batch_size=8)\n",
        "  val_data = NASTrainer.create_dummy_data(vocab_size=vocab_size, seq_len=seq_len, batch_size=4)\n",
        "\n",
        "  #Initialize trainer\n",
        "  trainer = NASTrainer(model, train_data, val_data, device='cpu')\n",
        "\n",
        "  #Training loop\n",
        "  num_epochs = 5 # You can change this to set the number of epochs\n",
        "  print(\"Starting Neural Architecture Search...\")\n",
        "  for epoch in range(num_epochs):\n",
        "    model_loss, arch_loss = trainer.train_epoch()\n",
        "    print(f\"Epoch {epoch}, Model Loss: {model_loss:.4f}, Arch Loss: {arch_loss:.4f}\")\n",
        "\n",
        "  #extract final architecture\n",
        "  final_arch = trainer.get_final_architecture()\n",
        "  print(\"\\nDiscovered Architecture:\")\n",
        "  print(f\"Optimal layers: {final_arch['optimal_layers']} \")\n",
        "  for i, block_config in enumerate(final_arch['block_configs'][:3]):\n",
        "    print(f\"Block {i}: {block_config}\")\n",
        "\n",
        "main()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Neural Architecture Search...\n",
            "Batch 0, Model Loss: 7.0470, Arch Loss: 0.0000\n",
            "Epoch 0, Model Loss: 7.0470, Arch Loss: 0.0000\n",
            "Batch 0, Model Loss: 6.9650, Arch Loss: 0.0000\n",
            "Epoch 1, Model Loss: 6.9650, Arch Loss: 0.0000\n",
            "Batch 0, Model Loss: 6.8759, Arch Loss: 0.0000\n",
            "Epoch 2, Model Loss: 6.8759, Arch Loss: 0.0000\n",
            "Batch 0, Model Loss: 6.8074, Arch Loss: 0.0000\n",
            "Epoch 3, Model Loss: 6.8074, Arch Loss: 0.0000\n",
            "Batch 0, Model Loss: 6.7370, Arch Loss: 0.0000\n",
            "Epoch 4, Model Loss: 6.7370, Arch Loss: 0.0000\n",
            "\n",
            "Discovered Architecture:\n",
            "Optimal layers: 4 \n",
            "Block 0: {'skip_pattern': 'postnorm', 'num_heads': 3, 'attention_pattern': 'sparse', 'ffn_expansion': 8}\n",
            "Block 1: {'skip_pattern': 'nonorm', 'num_heads': 1, 'attention_pattern': 'full', 'ffn_expansion': 6}\n",
            "Block 2: {'skip_pattern': 'nonorm', 'num_heads': 2, 'attention_pattern': 'full', 'ffn_expansion': 2}\n"
          ]
        }
      ]
    }
  ]
}